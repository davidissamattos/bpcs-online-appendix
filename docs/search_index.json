[["index.html", "Online appendix for the paper: “Bayesian Paired-Comparison with the bpcs package” Chapter 1 Foreword 1.1 Installation of the bpcs package 1.2 Session info", " Online appendix for the paper: “Bayesian Paired-Comparison with the bpcs package” David Issa Mattos and Érika Martins Silva Ramos 2021-06-08 Chapter 1 Foreword This is the online appendix for the paper “Bayesian Paired-Comparison with the bpcs package”. It contains a commented and reproducible code for all the analysis, tables and plots presented in the paper. In the beginning of each study, we show a few lines of the original datasets. These datasets are either available through a link in the paper or through asking the authors directly. Therefore we do not provide or distribute the data in this appendix (or in the code repository). 1.1 Installation of the bpcs package This appendix was compiled with the version 1.2.1 of the bpcs package. The development and latest version of the bpcs package can be installed directly from Github with: remotes::install_github(&#39;davidissamattos/bpcs&#39;) 1.2 Session info This appendix is compiled automatically and the following session information was used to generate this appendix: sessionInfo() ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] knitr_1.33 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.6 ## [5] purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.2 ## [9] ggplot2_3.3.3 tidyverse_1.3.1 bpcs_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.1 xfun_0.23 bslib_0.2.5.1 haven_2.4.1 ## [5] colorspace_2.0-1 vctrs_0.3.8 generics_0.1.0 htmltools_0.5.1.1 ## [9] yaml_2.2.1 utf8_1.2.1 rlang_0.4.11 jquerylib_0.1.4 ## [13] pillar_1.6.1 withr_2.4.2 glue_1.4.2 DBI_1.1.1 ## [17] dbplyr_2.1.1 readxl_1.3.1 modelr_0.1.8 lifecycle_1.0.0 ## [21] cellranger_1.1.0 munsell_0.5.0 gtable_0.3.0 rvest_1.0.0 ## [25] evaluate_0.14 fansi_0.5.0 broom_0.7.6 Rcpp_1.0.6 ## [29] scales_1.1.1 backports_1.2.1 jsonlite_1.7.2 fs_1.5.0 ## [33] hms_1.1.0 digest_0.6.27 stringi_1.6.2 bookdown_0.22 ## [37] grid_4.1.0 cli_2.5.0 tools_4.1.0 magrittr_2.0.1 ## [41] sass_0.4.0 crayon_1.4.1 pkgconfig_2.0.3 ellipsis_0.3.2 ## [45] xml2_1.3.2 reprex_2.0.0 lubridate_1.7.10 assertthat_0.2.1 ## [49] rmarkdown_2.8 httr_1.4.2 rstudioapi_0.13 R6_2.5.0 ## [53] compiler_4.1.0 "],["basics.html", "Chapter 2 Basic functionality 2.1 Reading and preparing the data 2.2 The Bayesian Bradley-Terry analysis", " Chapter 2 Basic functionality Although a large overview of the functionality of the bpcs is available in the official documentation (https://davidissamattos.github.io/bpcs) we provide here a short example based on the paper: Luckett, Curtis R., Sara L. Burns, and Lindsay Jenkinson. “Estimates of relative acceptability from paired preference tests.” Journal of Sensory Studies 35.5 (2020): e12593. library(bpcs) library(tidyverse) library(knitr) library(posterior) library(bayesplot) set.seed(99) 2.1 Reading and preparing the data This paper analyzes food preferences using paired comparisons (and compare different methods). The original data was made available in the paper and it can be found in the link: d &lt;- readxl::read_xlsx(path=&#39;data/PREF_DATA.xlsx&#39;, sheet = &#39;Pizza&#39;) Below we see a fragment of how the dataset looks like: sample_n(d, size=5) %&gt;% kable(caption=&quot;Example of the pizza data frame&quot;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) Table 2.1: Example of the pizza data frame Prod1 Prod2 Win1 Win2 Tombstone Red Barron 16 21 Freschetta DiGiorno 20 18 aKroger Tombstone 17 23 Freschetta Tombstone 22 16 DiGiorno aKroger 26 13 This data is in a aggregated format. That is each row contains more than one observation. For example, the first row shows that Tombstone was voted 16 times against Red Barron and Red Barron was voted 21 times against Tombstone To use with the bpcs package, we need to expand it to a single contest per row, in a long format. We can use the function expand_aggregated_data of the bpcs package exactly for this purpose. This leads to a data frame with 380 rows (same number of wins for 1 and 2). This function adds an id column to the data, so each row is uniquely represented (important if you will do some transformations later). Below we examplify how the expanded data looks like dpizza &lt;- expand_aggregated_data(d = d, player0 = &#39;Prod1&#39;, player1 = &#39;Prod2&#39;, wins0 = &#39;Win1&#39;, wins1 = &#39;Win2&#39;, keep=NULL) # renaming the columns colnames(dpizza) &lt;- c(&#39;Prod0&#39;,&#39;Prod1&#39;, &#39;y&#39;, &#39;contest_id&#39;) # creating a short table to exemplify sample_n(dpizza, size = 10) %&gt;% kable(caption = &#39;Sample of the expanded pizza data set&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) (#tab:expand_data2)Sample of the expanded pizza data set Prod0 Prod1 y contest_id aKroger Red Barron 1 362 Freschetta DiGiorno 1 226 DiGiorno Red Barron 0 128 aKroger Red Barron 1 358 Red Barron Freschetta 0 269 Tombstone Red Barron 1 20 DiGiorno Tombstone 1 68 Freschetta aKroger 1 265 Freschetta DiGiorno 0 210 DiGiorno aKroger 0 88 2.2 The Bayesian Bradley-Terry analysis Now that we have the data in the correct format we can use bpcs package to model the Bayesian Bradley-Terry Model. It is a good practice to save the result fitted model in a file right after sampling. Some models might take several minutes to fit and you probably don’t want to keep re-fitting the model always. After saving you can just read the model and continue your analysis instead of re-fitting. The save_bpc_model is a wrapper function around the saveRDS function with a few smaller checks. Few free to use any. To read you can use the load_bpc_model or the readRDS functions. Let’s run the simplest Bayesian Bradley-Terry model: m &lt;- bpc(data = dpizza, player0 = &#39;Prod0&#39;, player1 = &#39;Prod1&#39;, result_column = &#39;y&#39;, solve_ties = &#39;none&#39;, model_type = &#39;bt&#39;, iter=3000, show_chain_messages = T) save_bpc_model(m, &#39;pizza&#39;,&#39;.bpcs&#39;) To load: m &lt;- load_bpc_model(&#39;.bpcs/pizza.RDS&#39;) 2.2.1 Diagnostics After sampling, we can investigate the convergence of the model and the predictive posterior with shinystan. Convergence checks are already available in the fitted model, but for the posterior checks we need to first calculate the posterior predictive values with the posterior_predictive function. This function returns a list with two values, the y (original fitted values) and the y_pred (posterior predictve). Both are in the correct format to use with shinystan. We save them to the global environment and then we load it in shinystan directly (through the GUI). #posterior predictive check y_pp &lt;- posterior_predictive(m) y &lt;- y_pp$y y_pred &lt;- y_pp$y_pred launch_shinystan(m) Since everything looks fine we can proceed with the analysis. Alternatively, as a quick check we can use: check_convergence_diagnostics(m) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021351-1-6490e6.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021351-2-6490e6.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021351-3-6490e6.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021351-4-6490e6.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. Traceplots draws&lt;- m$fit$draws(variables = c(&#39;lambda&#39;)) mcmc_trace(posterior::as_draws(draws)) + labs(title=&quot;Traceplots&quot;) 2.2.2 Summary information The summary function in the command line provides some tables that help understand the model, such as the summary of the parameters, the probability of winning, and the ranking table. summary(m) ## Estimated baseline parameters with 95% HPD intervals: ## ## Table: (\\#tab:unnamed-chunk-11)Parameters estimates ## ## Parameter Mean Median HPD_lower HPD_higher ## ------------------- ------ ------- ---------- ----------- ## lambda[Tombstone] -0.14 -0.11 -2.77 2.57 ## lambda[DiGiorno] 0.33 0.34 -2.30 3.03 ## lambda[Freschetta] 0.22 0.25 -2.42 2.97 ## lambda[Red Barron] 0.24 0.26 -2.41 2.95 ## lambda[aKroger] -0.35 -0.32 -2.97 2.39 ## NOTES: ## * A higher lambda indicates a higher team ability ## ## Posterior probabilities: ## These probabilities are calculated from the predictive posterior distribution ## for all player combinations ## ## ## Table: (\\#tab:unnamed-chunk-11)Estimated posterior probabilites ## ## i j i_beats_j j_beats_i ## ----------- ----------- ---------- ---------- ## aKroger DiGiorno 0.43 0.57 ## aKroger Freschetta 0.32 0.68 ## aKroger Red Barron 0.39 0.61 ## aKroger Tombstone 0.41 0.59 ## DiGiorno Freschetta 0.55 0.45 ## DiGiorno Red Barron 0.45 0.55 ## DiGiorno Tombstone 0.69 0.31 ## Freschetta Red Barron 0.52 0.48 ## Freschetta Tombstone 0.57 0.43 ## Red Barron Tombstone 0.59 0.41 ## ## Rank of the players&#39; abilities: ## The rank is based on the posterior rank distribution of the lambda parameter ## ## Table: (\\#tab:unnamed-chunk-11)Estimated posterior ranks ## ## Parameter MedianRank MeanRank StdRank ## ----------- ----------- --------- -------- ## DiGiorno 1 1.64 0.77 ## Freschetta 2 2.24 0.84 ## Red Barron 2 2.22 0.85 ## Tombstone 4 4.07 0.52 ## aKroger 5 4.84 0.37 The presented tables can also be obtained individually as data frame. Additionally, for all of them it is possible to obtain the posterior distribution. The package has some publication-ready functions to create plots and tables. The format option can set it to latex, html, pandoc etc. Parameters with HPDI get_parameters_table(m, format=&#39;html&#39;, caption = &#39;Parameter estimates with HPDI&#39;, digits = 3) Table 2.2: Parameter estimates with HPDI Parameter Mean Median HPD_lower HPD_higher lambda[Tombstone] -0.136 -0.113 -2.775 2.573 lambda[DiGiorno] 0.329 0.344 -2.304 3.026 lambda[Freschetta] 0.223 0.249 -2.425 2.970 lambda[Red Barron] 0.240 0.261 -2.412 2.949 lambda[aKroger] -0.348 -0.318 -2.974 2.387 Probabilities get_probabilities_table(m, format=&#39;html&#39;, caption=&#39;Probability of selectiig a brand of pizza over the other&#39;) Table 2.3: Probability of selectiig a brand of pizza over the other i j i_beats_j j_beats_i aKroger DiGiorno 0.33 0.67 aKroger Freschetta 0.31 0.69 aKroger Red Barron 0.33 0.67 aKroger Tombstone 0.46 0.54 DiGiorno Freschetta 0.50 0.50 DiGiorno Red Barron 0.55 0.45 DiGiorno Tombstone 0.69 0.31 Freschetta Red Barron 0.46 0.54 Freschetta Tombstone 0.57 0.43 Red Barron Tombstone 0.61 0.39 Rank get_rank_of_players_table(m, caption=&#39;Rank of Pizza&#39;, format=&#39;html&#39;) Table 2.4: Rank of Pizza Parameter MedianRank MeanRank StdRank DiGiorno 1 1.666 0.801 Freschetta 2 2.314 0.847 Red Barron 2 2.129 0.837 Tombstone 4 4.046 0.546 aKroger 5 4.845 0.370 Plot plot(m, rotate_x_labels=T) We can also obtain information criteria to compare different models that fit the same data. Below we show how to obtain the WAIC and the LOO-CV get_waic(m) ## ## Computed from 12000 by 380 log-likelihood matrix ## ## Estimate SE ## elpd_waic -259.8 3.9 ## p_waic 4.0 0.1 ## waic 519.6 7.8 get_loo(m) ## ## Computed from 12000 by 380 log-likelihood matrix ## ## Estimate SE ## elpd_loo -259.8 3.9 ## p_loo 4.0 0.1 ## looic 519.6 7.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. This chapter show some basic functionality of the package. The next chapters will show how to apply these functions to solve more interesting and complex problems. We also recommend checking the package documentation at: https://davidissamattos.github.io/bpcs/index.html "],["studyI.html", "Chapter 3 Re-analysis 1 3.1 Importing the data 3.2 Analysis with the Bradley-Terry model and the order effect model 3.3 Diagnostics", " Chapter 3 Re-analysis 1 This reanalysis is based on the paper: Iwasa, Kazunori, et al. “Visual perception of moisture is a pathogen detection mechanism of the behavioral immune system.” Frontiers in Psychology 11 (2020): 170. The data of this paper can be obtained from the repository: https://osf.io/5quj9/ library(bpcs) library(tidyverse) library(knitr) set.seed(99) 3.1 Importing the data First let’s read the data: d &lt;- readxl::read_xlsx(&#39;data/moisture.xlsx&#39;, sheet = &#39;Exp02_BTmodel&#39;) Below we show a sample of how the original data looks like: sample_n(d, size = 5) %&gt;% kable(caption = &#39;Sample of the original data&#39;) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 3.1: Sample of the original data player1 player2 win1 win2 image7 image6 131 109 image5 image6 18 222 image7 image2 226 14 image4 image1 239 1 image5 image8 20 220 The data is in the aggregated format. So let’s expand it to the long format d_moisture &lt;- expand_aggregated_data(d, player0 = &#39;player1&#39;, player1=&#39;player2&#39;, wins0 = &#39;win1&#39;, wins1=&#39;win2&#39;) Now the data looks like this: sample_n(d_moisture, size = 20) %&gt;% kable(caption = &#39;The data in the long format&#39;) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 3.2: The data in the long format player0 player1 y rowid image2 image7 1 2922 image5 image2 0 7102 image7 image6 1 11490 image2 image8 1 3200 image1 image3 1 358 image8 image4 1 12704 image6 image3 0 8973 image8 image3 0 12308 image6 image4 0 9188 image5 image2 0 7071 image5 image1 0 6724 image4 image7 1 6409 image8 image1 0 11986 image4 image1 0 5278 image3 image7 1 4694 image4 image5 1 5972 image1 image8 1 1533 image4 image8 1 6560 image1 image3 1 398 image3 image5 1 4228 3.2 Analysis with the Bradley-Terry model and the order effect model Although this is multiple judgment case, the dataset in the aggregated format does not provide information of how each individual voted, therefore we cannot compensate this effect. Therefore, we will create an analysis with only the Bradley-Terry model and the Bradley-Terry model with order effect First, let’s sample the Bradley-Terry model m_moisture &lt;- bpc( d_moisture, player0 = &#39;player0&#39;, player1 = &#39;player1&#39;, result_column = &#39;y&#39;, model_type = &#39;bt&#39;, iter = 2000, show_chain_messages = T ) save_bpc_model(m_moisture,&#39;m_moisture&#39;,&#39;./fittedmodels&#39;) Low let’s sample the model with order effect. Although the authors said that the order of the images was inverted to compensate order effect, we can still estimate if there is an order effect or not in the choice. But first we need to create a column indicating if there was order effect for that case or not. In this problem, we just indicate with a column of ones that all instances could have an order effect. Not that the package marks the order effect relative to player1. So if the values should be interpret as such. d_moisture &lt;- d_moisture %&gt;% dplyr::mutate(z1 = 1) m_moisture_order &lt;- bpc( d_moisture, player0 = &#39;player0&#39;, player1 = &#39;player1&#39;, result_column = &#39;y&#39;, z_player1 = &#39;z1&#39;, model_type = &#39;bt-ordereffect&#39;, iter = 2000, show_chain_messages = T ) save_bpc_model(m_moisture_order,&#39;m_moisture_order&#39;,&#39;./fittedmodels&#39;) 3.3 Diagnostics check_convergence_diagnostics(m_moisture) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106040755-1-4da89a.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106040755-2-4da89a.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106040755-3-4da89a.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106040755-4-4da89a.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. check_convergence_diagnostics(m_moisture_order) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106041018-1-160b7f.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106041018-2-160b7f.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106041018-3-160b7f.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106041018-4-160b7f.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. Checking convergence of the models with shinystan launch_shinystan(m_moisture) launch_shinystan(m_moisture_order) 3.3.1 Tables and plots First, lets get a table for the parameters (to export it to Latex just utilize the format option) get_parameters_table(m_moisture, format = &#39;html&#39;, caption = &#39;Parameters estimates for the simple Bradley-Terry model&#39;) Table 3.3: Parameters estimates for the simple Bradley-Terry model Parameter Mean Median HPD_lower HPD_higher lambda[image1] -4.577 -4.554 -6.533 -2.489 lambda[image2] -2.465 -2.436 -4.503 -0.461 lambda[image3] -0.154 -0.120 -2.221 1.830 lambda[image4] 0.038 0.069 -2.065 1.977 lambda[image5] 0.197 0.227 -1.874 2.175 lambda[image6] 1.742 1.770 -0.349 3.668 lambda[image7] 1.917 1.947 -0.188 3.842 lambda[image8] 2.930 2.967 0.879 4.920 get_parameters_table( m_moisture_order, params = c(&#39;lambda&#39;, &#39;gm&#39;), format = &#39;html&#39;, caption = &#39;Parameters estimates for the Bradley-Terry model with order effect&#39;, keep_par_name = F, n_eff = T ) Table 3.4: Parameters estimates for the Bradley-Terry model with order effect Parameter Mean Median HPD_lower HPD_higher n_eff image1 -4.560 -4.527 -6.490 -2.425 894.5013 image2 -2.445 -2.414 -4.489 -0.463 883.5518 image3 -0.133 -0.095 -2.137 1.876 880.3234 image4 0.059 0.094 -1.966 2.050 880.4426 image5 0.218 0.255 -1.795 2.220 879.9749 image6 1.761 1.793 -0.230 3.772 881.7298 image7 1.938 1.975 -0.023 3.986 882.7975 image8 2.951 2.989 0.971 4.987 881.5544 gm -0.001 0.000 -0.054 0.056 2422.1624 We can see from the table of the order effect that the gamma parameter is very close to zero, indicating that there is no order effect Now lets compute the posterior ranks of the images based on the first BT model r &lt;- get_rank_of_players_df(m_moisture) Generating a table with the images r %&gt;% mutate(Image = c(&quot;data/moisture/Stimuli/image08.jpg&quot;, &quot;data/moisture/Stimuli/image07.jpg&quot;, &quot;data/moisture/Stimuli/image06.jpg&quot;, &quot;data/moisture/Stimuli/image05.jpg&quot;, &quot;data/moisture/Stimuli/image04.jpg&quot;, &quot;data/moisture/Stimuli/image03.jpg&quot;, &quot;data/moisture/Stimuli/image02.jpg&quot;, &quot;data/moisture/Stimuli/image01.jpg&quot;) %&gt;% pander::pandoc.image.return()) %&gt;% knitr::kable(caption = &quot;Rank of the images based on moisture content&quot;, format=&#39;html&#39;, booktabs=T) Table 3.5: Rank of the images based on moisture content Parameter MedianRank MeanRank StdRank Image image8 1 1.000 0.0000000 image7 2 2.003 0.0547174 image6 3 2.997 0.0547174 image5 4 4.005 0.0705690 image4 5 4.996 0.0773950 image3 6 5.999 0.0316228 image2 7 7.000 0.0000000 image1 8 8.000 0.0000000 Now lets get a caterpillar style plot plot( m_moisture_order, HPDI = T, title = &#39;Estimates of the moisture content&#39;, xaxis = &#39;Images&#39;, yaxis = &#39;Ability&#39;, rotate_x_labels = F, APA = F, keep_par_names=F) + theme_bw() 3.3.2 WAIC Calculating the WAIC of both models get_waic(m_moisture) ## ## Computed from 8000 by 13440 log-likelihood matrix ## ## Estimate SE ## elpd_waic -4066.1 78.8 ## p_waic 6.9 0.2 ## waic 8132.2 157.6 get_waic(m_moisture_order) ## ## Computed from 8000 by 13440 log-likelihood matrix ## ## Estimate SE ## elpd_waic -4067.0 78.8 ## p_waic 7.8 0.2 ## waic 8134.1 157.7 We can see that the WAIC of the models are quite similar and that the model without the order effect has a slightly smaller WAIC and less parameters. Therefore we will select it. "],["studyII.html", "Chapter 4 Re-analysis 2 4.1 Importing the data 4.2 Simple Bradley-Terry model 4.3 Bradley-Terry model with random effects for individuals", " Chapter 4 Re-analysis 2 This reanalysis is based on the paper: Huskisson, S.M., Jacobson, S.L., Egelkamp, C.L. et al. Using a Touchscreen Paradigm to Evaluate Food Preferences and Response to Novel Photographic Stimuli of Food in Three Primate Species (Gorilla gorilla gorilla, Pan troglodytes, and Macaca fuscata). Int J Primatol 41, 5–23 (2020). https://doi.org/10.1007/s10764-020-00131-0 library(bpcs) library(tidyverse) library(knitr) set.seed(99) 4.1 Importing the data The data from this paper was made available upon request and below we exemplify a few rows of how the original dataset looks like d &lt;- readr::read_csv(&#39;data/touchscreen.csv&#39;) Previewing how the data looks like dplyr::sample_n(d, size = 10) %&gt;% knitr::kable(caption=&#39;Sample of how the dataset looks like&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.1: Sample of how the dataset looks like date species_type SubjectCode Sex Trial image1 image2 image_chosen concat_test_name 1/22/18 Macaque Macaque7 Female 1 Ca Pe Pe CaPe 1/5/17 Gorilla Gorilla4 Male 28 Ca Tu Tu CaTu 3/3/17 Chimpanzee Chimpanzee1 Female 19 Cu Gr Gr CuGr 12/4/17 Chimpanzee Chimpanzee1 Female 27 Ap Tu Tu ApTu 5/1/17 Gorilla Gorilla6 Male 14 Ca Tu Ca CaTu 8/6/18 Gorilla Gorilla5 Female 3 Cu To To CuTo 1/31/18 Gorilla Gorilla5 Female 5 Ca Cu Ca CaCu 8/3/17 Gorilla Gorilla6 Male 8 Ca To To CaTo 4/18/17 Gorilla Gorilla2 Male 5 Ca To To CaTo 3/20/17 Gorilla Gorilla2 Male 9 Gr To Gr GrTo Now we need to modify a bit the data frame to create a column with the results as 0 and 1. Creating a numerical result vector with 0 for image1 and 1 for image2 d &lt;- d %&gt;% dplyr::mutate(y =ifelse(image_chosen==image1, 0, 1)) Adding names to the abbreviations #image1 d$image1 &lt;- dplyr::recode(d$image1, &quot;Ca&quot; = &#39;Carrot&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Cu&quot; = &#39;Cucumber&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Tu&quot; = &#39;Turnip&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Gr&quot; = &#39;Grape&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;To&quot; = &#39;Tomato&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Ap&quot; = &#39;Apple&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Jp&quot; = &#39;Jungle Pellet&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Ce&quot; = &#39;Celery&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Gb&quot; = &#39;Green Beans&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Oa&quot; = &#39;Oats&#39;) d$image1 &lt;- dplyr::recode(d$image1, &quot;Pe&quot; = &#39;Peanuts&#39;) #image2 d$image2 &lt;- dplyr::recode(d$image2, &quot;Ca&quot; = &#39;Carrot&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Cu&quot; = &#39;Cucumber&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Tu&quot; = &#39;Turnip&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Gr&quot; = &#39;Grape&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;To&quot; = &#39;Tomato&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Ap&quot; = &#39;Apple&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Jp&quot; = &#39;Jungle Pellet&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Ce&quot; = &#39;Celery&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Gb&quot; = &#39;Green Beans&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Oa&quot; = &#39;Oats&#39;) d$image2 &lt;- dplyr::recode(d$image2, &quot;Pe&quot; = &#39;Peanuts&#39;) Separating the data into three datasets. One for each species. macaque &lt;- d %&gt;% dplyr::filter(species_type==&#39;Macaque&#39;) chip &lt;- d %&gt;% dplyr::filter(species_type==&#39;Chimpanzee&#39;) gor &lt;- d %&gt;% dplyr::filter(species_type==&#39;Gorilla&#39;) Below we show a few lines of each dataset: dplyr::sample_n(gor, size = 10) %&gt;% knitr::kable(caption=&#39;Sample of how the gorilla dataset looks like&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.2: Sample of how the gorilla dataset looks like date species_type SubjectCode Sex Trial image1 image2 image_chosen concat_test_name y 8/3/17 Gorilla Gorilla6 Male 14 Carrot Tomato To CaTo 1 8/23/17 Gorilla Gorilla5 Female 24 Cucumber Grape Gr CuGr 1 12/7/16 Gorilla Gorilla2 Male 28 Cucumber Turnip Cu CuTu 0 5/25/17 Gorilla Gorilla1 Male 4 Carrot Cucumber Ca CaCu 0 11/3/16 Gorilla Gorilla4 Male 2 Carrot Grape Ca CaGr 0 7/12/17 Gorilla Gorilla3 Male 3 Grape Tomato Gr GrTo 0 1/24/17 Gorilla Gorilla5 Female 7 Carrot Grape Ca CaGr 0 11/15/17 Gorilla Gorilla1 Male 8 Grape Tomato Gr GrTo 0 11/22/17 Gorilla Gorilla6 Male 28 Apple Grape Gr ApGr 1 10/2/17 Gorilla Gorilla1 Male 19 Apple Carrot Ca ApCa 1 dplyr::sample_n(chip, size = 10) %&gt;% knitr::kable(caption=&#39;Sample of how the chimpanzees dataset looks like&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.3: Sample of how the chimpanzees dataset looks like date species_type SubjectCode Sex Trial image1 image2 image_chosen concat_test_name y 7/26/17 Chimpanzee Chimpanzee3 Female 20 Cucumber Grape Cu CuGr 0 10/26/17 Chimpanzee Chimpanzee1 Female 7 Apple Grape Gr ApGr 1 9/18/17 Chimpanzee Chimpanzee4 Male 27 Carrot Grape Gr CaGr 1 12/8/16 Chimpanzee Chimpanzee1 Female 12 Cucumber Turnip Cu CuTu 0 12/14/17 Chimpanzee Chimpanzee3 Female 14 Tomato Turnip To ToTu 0 1/12/17 Chimpanzee Chimpanzee4 Male 21 Cucumber Turnip Tu CuTu 1 6/14/17 Chimpanzee Chimpanzee3 Female 14 Carrot Cucumber Ca CaCu 0 9/19/17 Chimpanzee Chimpanzee4 Male 19 Carrot Grape Ca CaGr 0 3/5/18 Chimpanzee Chimpanzee4 Male 22 Apple Carrot Ap ApCa 0 2/7/17 Chimpanzee Chimpanzee2 Female 28 Grape Turnip Gr GrTu 0 dplyr::sample_n(macaque, size = 10) %&gt;% knitr::kable(caption=&#39;Sample of how the macaques dataset looks like&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.4: Sample of how the macaques dataset looks like date species_type SubjectCode Sex Trial image1 image2 image_chosen concat_test_name y 05-04-2018@11-27 Macaque Macaque1 Male 1 Celery Green Beans Gb GbCe 1 1/16/18 Macaque Macaque7 Female 14 Carrot Peanuts Ca CaPe 0 04-02-2018@11-27 Macaque Macaque1 Male 28 Carrot Green Beans Ca GbCa 0 9/7/17 Macaque Macaque6 Female 2 Carrot Celery Ca CeCa 0 8/14/17 Macaque Macaque2 Female 14 Carrot Peanuts Pe CaPe 1 5/24/18 Macaque Macaque4 Female 17 Jungle Pellet Oats Jp JpOa 0 02-12-2018@11-48 Macaque Macaque1 Male 24 Peanuts Celery Ce CePe 1 3/26/18 Macaque Macaque7 Female 10 Carrot Celery Ca CeCa 0 8/4/17 Macaque Macaque4 Female 2 Jungle Pellet Peanuts Jp JpPe 0 4/19/18 Macaque Macaque4 Female 17 Peanuts Oats Pe PeOa 0 4.2 Simple Bradley-Terry model Now that the data is ready let’s fit three simple Bayesian Bradley-Terry models m1_macaque &lt;- bpc( macaque, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, result_column = &#39;y&#39;, model_type = &#39;bt&#39;, priors = list(prior_lambda_std = 3.0), iter = 3000, show_chain_messages = T ) save_bpc_model(m1_macaque, &#39;m1_macaque&#39;, &#39;./fittedmodels&#39;) m1_chip &lt;- bpc( chip, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, result_column = &#39;y&#39;, model_type = &#39;bt&#39;, priors = list(prior_lambda_std = 3.0), iter = 3000, show_chain_messages = T ) save_bpc_model(m1_chip, &#39;m1_chip&#39;, &#39;./fittedmodels&#39;) m1_gor &lt;- bpc( gor, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, result_column = &#39;y&#39;, model_type = &#39;bt&#39;, priors = list(prior_lambda_std = 3.0), iter = 3000, show_chain_messages = T ) save_bpc_model(m1_gor, &#39;m1_gor&#39;, &#39;./fittedmodels&#39;) 4.2.1 Assessing the fitness of the model Here we are illustrating how to conduct the diagnostic analysis for one model only. Since the shinystan app does not appear in the compiled appendix we are just representing it here once for the Chimpanzees model. Note that it is still possible to use the bayesplot package to generate static figures if needed. #First we get the posterior predictive in the environment pp_m1_chip &lt;- posterior_predictive(m1_chip, n = 100) y_pp_m1_chip &lt;- pp_m1_chip_gor$y ypred_pp_m1_chip &lt;- pp_m1_chip$y_pred Then we launch shinystan to assess convergence and validity of the model (e.g.) launch_shinystan(m1_chip) We can also do some quick checks with: check_convergence_diagnostics(m1_chip) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021838-1-7618ae.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021838-2-7618ae.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021838-3-7618ae.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021838-4-7618ae.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. check_convergence_diagnostics(m1_macaque) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021755-1-43f436.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021755-2-43f436.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021755-3-43f436.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021755-4-43f436.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. check_convergence_diagnostics(m1_gor) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021909-1-2d1bbd.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021909-2-2d1bbd.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021909-3-2d1bbd.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021909-4-2d1bbd.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. 4.2.2 Getting the WAIC Before we start getting the parameters tables and etc let’s get the WAIC so we can compare with the next model (with random effects) get_waic(m1_macaque) ## ## Computed from 12000 by 8400 log-likelihood matrix ## ## Estimate SE ## elpd_waic -3550.7 56.7 ## p_waic 5.0 0.1 ## waic 7101.5 113.4 get_waic(m1_chip) ## ## Computed from 12000 by 5400 log-likelihood matrix ## ## Estimate SE ## elpd_waic -3599.7 17.0 ## p_waic 5.0 0.0 ## waic 7199.4 33.9 get_waic(m1_gor) ## ## Computed from 12000 by 8100 log-likelihood matrix ## ## Estimate SE ## elpd_waic -4883.6 35.9 ## p_waic 4.9 0.0 ## waic 9767.2 71.9 4.3 Bradley-Terry model with random effects for individuals Let’s add the cluster SubjectCode as a random effects in our model m2_macaque &lt;- bpc( macaque, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, result_column = &#39;y&#39;, model_type = &#39;bt-U&#39;, cluster = c(&#39;SubjectCode&#39;), priors = list( prior_lambda_std = 1.0, prior_U1_std = 1.0 ), iter = 3000, show_chain_messages = T ) save_bpc_model(m2_macaque, &#39;m2_macaque&#39;, &#39;./fittedmodels&#39;) m2_chip &lt;- bpc( chip, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, cluster = c(&#39;SubjectCode&#39;), result_column = &#39;y&#39;, model_type = &#39;bt-U&#39;, priors = list( prior_lambda_std = 1.0, prior_U1_std = 1.0 ), iter = 3000, show_chain_messages = T ) save_bpc_model(m2_chip, &#39;m2_chip&#39;, &#39;./fittedmodels&#39;) m2_gor &lt;- bpc( gor, player0 = &#39;image1&#39;, player1 = &#39;image2&#39;, cluster = c(&#39;SubjectCode&#39;), result_column = &#39;y&#39;, model_type = &#39;bt-U&#39;, priors = list( prior_lambda_std = 1.0, prior_U1_std = 1.0 ), iter = 3000, show_chain_messages = T ) save_bpc_model(m2_gor, &#39;m2_gor&#39;, &#39;./fittedmodels&#39;) Of course we should run diagnostic analysis on the models. For the gorillas launch_shinystan(m2_gor) We can also do some quick checks with: check_convergence_diagnostics(m2_chip) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030050-1-3fa7ac.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030050-2-3fa7ac.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030050-3-3fa7ac.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030050-4-3fa7ac.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. check_convergence_diagnostics(m2_macaque) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021959-1-11945d.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021959-2-11945d.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021959-3-11945d.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106021959-4-11945d.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. check_convergence_diagnostics(m2_gor) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030410-1-75c08b.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030410-2-75c08b.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030410-3-75c08b.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106030410-4-75c08b.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. 4.3.1 Getting the WAIC Before we start plotting tables let’s get the WAIC for each random effects model and compare with the first models without random effects get_waic(m2_macaque) ## ## Computed from 12000 by 8400 log-likelihood matrix ## ## Estimate SE ## elpd_waic -3356.3 57.1 ## p_waic 32.7 0.8 ## waic 6712.6 114.2 get_waic(m2_chip) ## ## Computed from 12000 by 5400 log-likelihood matrix ## ## Estimate SE ## elpd_waic -3360.0 24.1 ## p_waic 19.7 0.3 ## waic 6720.0 48.3 get_waic(m2_gor) ## ## Computed from 12000 by 8100 log-likelihood matrix ## ## Estimate SE ## elpd_waic -4396.5 40.1 ## p_waic 29.5 0.4 ## waic 8792.9 80.2 Below I just copied the result of the WAIC into a data frame to create the tables. Of course this process could be automated. waic_table &lt;- data.frame( Species = c(&#39;Macaques&#39;, &#39;Chimpanzees&#39;, &#39;Gorillas&#39;), BT = c(7101.5, 7199.4, 9767.2), BTU = c(6712.6, 6720.0, 8792.9) ) The kableExtra package provides some nice tools to create tables from R directly to Latex kable( waic_table, booktabs=T, caption = &#39;Comparison of the WAIC of the Bradley-Terry model and the Bradley-Terry model with random effects on the subjects for each specie.&#39;, col.names = c(&#39;Specie&#39;, &#39;Bradley-Terry&#39;, &#39;Bradley-Terry with random effects&#39;) ) %&gt;% kableExtra::add_header_above(c(&quot; &quot; = 1, &quot;WAIC&quot; = 2)) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.5: Comparison of the WAIC of the Bradley-Terry model and the Bradley-Terry model with random effects on the subjects for each specie. WAIC Specie Bradley-Terry Bradley-Terry with random effects Macaques 7101.5 6712.6 Chimpanzees 7199.4 6720.0 Gorillas 9767.2 8792.9 We can see that the random effects model perform much better than the simple BT model by having a much lower WAIC. Therefore from now we will use only the random effects model to generate our tables and plots 4.3.2 Parameter tables and plots Now let’s create some plots and tables to analyze and compare the models 4.3.2.1 Parameters table Creating a nice table of the parameters. First let’s put all species in the same data frame df1 &lt;- get_parameters(m2_macaque, n_eff = T, keep_par_name = F) df2 &lt;- get_parameters(m2_chip, n_eff = T, keep_par_name = F) df3 &lt;- get_parameters(m2_gor, n_eff = T, keep_par_name = F) df1 &lt;- df1 %&gt;% dplyr::mutate(Species=&#39;Macaque&#39;) df2 &lt;- df2 %&gt;% dplyr::mutate(Species=&#39;Chimpanzees&#39;) df3 &lt;- df3 %&gt;% dplyr::mutate(Species=&#39;Gorilla&#39;) #appending the dataframes df &lt;- rbind(df1,df2,df3) #Removing the individual random effects parameters otherwise the table will be much bigger df &lt;- df %&gt;% filter(!startsWith(Parameter,&#39;U1[&#39;)) rownames(df) &lt;- NULL Now let’s create the table by removing the species column and adding some row Headers for the species (df %&gt;% select(-Species) %&gt;% kable( caption = &#39;Parameters of the random effects model with 95% HPD and the number of effective samples.&#39;, digits = 2, col.names = c(&#39;Parameter&#39;, &#39;Mean&#39;, &#39;Median&#39;, &#39;HPD lower&#39;, &#39;HPD upper&#39;, &#39;N. Eff. Samples&#39;), row.names = NA) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::pack_rows(&quot;Macaque&quot;, 1, 6) %&gt;% kableExtra::pack_rows(&quot;Chimpanzees&quot;, 7, 13) %&gt;% kableExtra::pack_rows(&quot;Gorilla&quot;, 14, 20) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) ) Table 4.6: Parameters of the random effects model with 95% HPD and the number of effective samples. Parameter Mean Median HPD lower HPD upper N. Eff. Samples Macaque Carrot 0.12 0.12 -0.74 1.04 9934 Celery -2.28 -2.29 -3.17 -1.39 9712 Jungle Pellet 1.23 1.23 0.36 2.15 9778 Oats -0.90 -0.90 -1.77 0.05 9886 Peanuts 2.01 2.00 1.14 2.92 10195 Green Beans -0.17 -0.17 -1.05 0.76 10176 Chimpanzees U1_std 0.58 0.57 0.42 0.75 4277 Apple 0.03 0.03 -0.97 1.08 10200 Tomato 0.32 0.32 -0.70 1.34 9880 Carrot -0.21 -0.21 -1.25 0.77 9435 Grape 0.62 0.62 -0.38 1.69 10186 Cucumber -0.32 -0.33 -1.36 0.67 9911 Turnip -0.43 -0.43 -1.43 0.59 9342 Gorilla U1_std 0.72 0.70 0.49 1.01 4675 Apple 0.03 0.03 -0.95 0.99 13186 Carrot -0.11 -0.11 -1.10 0.84 12365 Grape 0.86 0.87 -0.13 1.81 13237 Tomato 0.85 0.86 -0.12 1.85 13031 Cucumber -0.70 -0.70 -1.67 0.27 12674 Turnip -0.94 -0.94 -1.93 0.04 12944 U1_std 0.78 0.77 0.57 1.02 4883 4.3.2.2 Rank table Rank of the food preferences rank1 &lt;- get_rank_of_players_df(m2_macaque) rank2 &lt;- get_rank_of_players_df(m2_chip) rank3 &lt;- get_rank_of_players_df(m2_gor) #appending the dataframes rank_all &lt;- rbind(rank1,rank2,rank3) Now let’s create the rank table (rank_all %&gt;% kable( caption = &#39;Ranking of the food preferences per specie for the random effects model.&#39;, digits = 2, col.names = c(&#39;Food&#39;, &#39;Median Rank&#39;, &#39;Mean Rank&#39;, &#39;Std. Rank&#39;)) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::pack_rows(&quot;Macaque&quot;, 1, 6) %&gt;% kableExtra::pack_rows(&quot;Chimpanzees&quot;, 7, 12) %&gt;% kableExtra::pack_rows(&quot;Gorilla&quot;, 13, 18) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) ) Table 4.7: Ranking of the food preferences per specie for the random effects model. Food Median Rank Mean Rank Std. Rank Macaque Peanuts 1 1.01 0.09 Jungle Pellet 2 1.99 0.09 Carrot 3 3.16 0.37 Green Beans 4 3.85 0.38 Oats 5 4.99 0.09 Celery 6 6.00 0.00 Chimpanzees Grape 1 1.51 0.86 Tomato 2 2.26 1.10 Apple 3 3.36 1.26 Carrot 4 4.23 1.25 Cucumber 5 4.62 1.24 Turnip 5 5.02 1.10 Gorilla Tomato 1 1.54 0.58 Grape 2 1.57 0.61 Apple 3 3.37 0.68 Carrot 4 3.71 0.71 Cucumber 5 5.20 0.67 Turnip 6 5.63 0.57 4.3.2.3 Plot Now let’s use ggplot to create a plot comparing both types of model. The simple BT and the BT with Random effects First we need to prepare the data frames for plotting. For the simple BT model (called old) df1_old &lt;- get_parameters(m1_macaque, n_eff = F, keep_par_name = F) df2_old &lt;- get_parameters(m1_chip, n_eff = F, keep_par_name = F) df3_old &lt;- get_parameters(m1_gor, n_eff = F, keep_par_name = F) df1_old &lt;- df1_old %&gt;% dplyr::mutate(Species=&#39;Macaque&#39;, Model=&#39;Simple&#39;) df2_old &lt;- df2_old %&gt;% dplyr::mutate(Species=&#39;Chimpanzees&#39;, Model=&#39;Simple&#39;) df3_old &lt;- df3_old %&gt;% dplyr::mutate(Species=&#39;Gorilla&#39;, Model=&#39;Simple&#39;) #appending the dataframes df_old &lt;- rbind(df1_old,df2_old,df3_old) #Removing the individual random effects parameters df_old &lt;- df_old %&gt;% filter(!startsWith(Parameter,&#39;U&#39;)) rownames(df_old) &lt;- NULL For the BT with random effects df1 &lt;- get_parameters(m2_macaque, n_eff = F, keep_par_name = F) df2 &lt;- get_parameters(m2_chip, n_eff = F, keep_par_name = F) df3 &lt;- get_parameters(m2_gor, n_eff = F, keep_par_name = F) df1 &lt;- df1 %&gt;% dplyr::mutate(Species=&#39;Macaque&#39;, Model=&#39;RandomEffects&#39;) df2 &lt;- df2 %&gt;% dplyr::mutate(Species=&#39;Chimpanzees&#39;, Model=&#39;RandomEffects&#39;) df3 &lt;- df3 %&gt;% dplyr::mutate(Species=&#39;Gorilla&#39;, Model=&#39;RandomEffects&#39;) #appending the dataframes df &lt;- rbind(df1,df2,df3) #Removing the individual random effects parameters df &lt;- df %&gt;% filter(!startsWith(Parameter,&#39;U&#39;)) rownames(df) &lt;- NULL Now we can merge them in a single data frame for ggplot #appending the dataframes out &lt;- rbind(df, df_old) #To order in ggplot we need to specify the order in the levels. We want to place the first model first and the random effects second out$Model&lt;-factor(out$Model, levels=c(&#39;Simple&#39;,&#39;RandomEffects&#39;)) # Defining a black-gray palette: cbp1 &lt;- c(&quot;#000000&quot;, &quot;#999999&quot;) #Using the pointrange function to define the HPD intervals ggplot(out, aes(x=Parameter))+ geom_pointrange(aes( ymin = HPD_lower, ymax = HPD_higher, y = Mean, group=Model, color=Model), position=position_dodge(width=1))+ #separating the two models (so they are not plotted overlapping) labs(title = &#39;Parameters estimates with the 95% HPD interval&#39;, y = &#39;Worth value&#39;, x = &#39;Food&#39;)+ facet_grid(~Species) + #Dividing the plot into three by species theme_bw()+ # A black and white theme # scale_x_discrete(guide = guide_axis(n.dodge = 2)) + theme(legend.position=&quot;bottom&quot;)+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+#small adjustments to the theme scale_colour_manual(values = cbp1) #applying the color palette 4.3.2.4 Probability of selecting a novel stimuli We will create a the table of the predictions of selecting a novel stimuli (compared to the trained ones). This is replication of table II of the original paper (of course the results are not the same since we are using different models and estimation values) For that, we first create a data frame of the new predictions for each species. Since our model uses random effects and we would need to specify each random effect to make the predictions we will do something slightly different. We will consider that the random effects will be zero, that is, which is equivalent to the average value of the random effects (remember that it has a mean of zero). One way to achieve this is by using the obtained coefficients in a submodel. This can be done in the bpcs package by using the model_type option. We ask for the data frame instead of the table because we will assemble the table manually. # Create a data frame with all the pairs of food that we want to calculate pairs_gor &lt;- data.frame( image2 = c( &#39;Apple&#39;, &#39;Apple&#39;, &#39;Apple&#39;, &#39;Apple&#39;, &#39;Tomato&#39;, &#39;Tomato&#39;, &#39;Tomato&#39;, &#39;Tomato&#39; ), image1 = c( &#39;Cucumber&#39;, &#39;Grape&#39;, &#39;Turnip&#39;, &#39;Carrot&#39;, &#39;Cucumber&#39;, &#39;Grape&#39;, &#39;Turnip&#39;, &#39;Carrot&#39; ) ) pairs_chip &lt;- data.frame( image2 = c( &#39;Apple&#39;, &#39;Apple&#39;, &#39;Apple&#39;, &#39;Apple&#39;, &#39;Tomato&#39;, &#39;Tomato&#39;, &#39;Tomato&#39;, &#39;Tomato&#39; ), image1 = c( &#39;Cucumber&#39;, &#39;Grape&#39;, &#39;Turnip&#39;, &#39;Carrot&#39;, &#39;Cucumber&#39;, &#39;Grape&#39;, &#39;Turnip&#39;, &#39;Carrot&#39; ) ) pairs_macaque &lt;- data.frame( image2 = c( &#39;Oats&#39;, &#39;Oats&#39;, &#39;Oats&#39;, &#39;Oats&#39;, &#39;Green Beans&#39;, &#39;Green Beans&#39;, &#39;Green Beans&#39;, &#39;Green Beans&#39; ), image1 = c( &#39;Celery&#39;, &#39;Jungle Pellet&#39;, &#39;Peanuts&#39;, &#39;Carrot&#39;, &#39;Celery&#39;, &#39;Jungle Pellet&#39;, &#39;Peanuts&#39;, &#39;Carrot&#39; ) ) prob_chip &lt;- get_probabilities_df( m2_chip, newdata = pairs_chip, model_type = &#39;bt&#39;) prob_macaque &lt;- get_probabilities_df( m2_macaque, newdata = pairs_macaque, model_type = &#39;bt&#39;) prob_gor &lt;- get_probabilities_df( m2_gor, newdata = pairs_gor, model_type = &#39;bt&#39;) #merging in a single df prob_table &lt;- rbind(prob_gor, prob_chip, prob_macaque) Now we can create the table prob_table &lt;- prob_table %&gt;% mutate(Probability = i_beats_j, OddsRatio = Probability / (1 - Probability)) prob_table %&gt;% dplyr::select(i, j, Probability, OddsRatio) %&gt;% kable( caption = &#39;Posterior probabilities of the novel stimuli i being selected over the trained stimuli j&#39;, booktabs = T, digits = 2, col.names = c(&#39;Item i&#39;, &#39;Item j&#39;, &#39;Probability&#39;, &#39;Odds Ratio&#39;) ) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::pack_rows(&quot;Gorilla&quot;, 1, 8) %&gt;% kableExtra::pack_rows(&quot;Chimpanzee&quot;, 9, 16) %&gt;% kableExtra::pack_rows(&quot;Macaque&quot;, 17, 24) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 4.8: Posterior probabilities of the novel stimuli i being selected over the trained stimuli j Item i Item j Probability Odds Ratio Gorilla Apple Cucumber 0.63 1.70 Apple Grape 0.33 0.49 Apple Turnip 0.80 4.00 Apple Carrot 0.63 1.70 Tomato Cucumber 0.83 4.88 Tomato Grape 0.51 1.04 Tomato Turnip 0.88 7.33 Tomato Carrot 0.76 3.17 Chimpanzee Apple Cucumber 0.64 1.78 Apple Grape 0.32 0.47 Apple Turnip 0.54 1.17 Apple Carrot 0.56 1.27 Tomato Cucumber 0.69 2.23 Tomato Grape 0.48 0.92 Tomato Turnip 0.72 2.57 Tomato Carrot 0.59 1.44 Macaque Oats Celery 0.78 3.55 Oats Jungle Pellet 0.10 0.11 Oats Peanuts 0.01 0.01 Oats Carrot 0.25 0.33 Green Beans Celery 0.93 13.29 Green Beans Jungle Pellet 0.17 0.20 Green Beans Peanuts 0.13 0.15 Green Beans Carrot 0.56 1.27 "],["studyIII.html", "Chapter 5 Re-analysis 3 5.1 Importing the data 5.2 Simple Bradley-Terry models 5.3 Subject predictors model 5.4 Subject predictors and random effects 5.5 Comparing the WAIC 5.6 Plots and tables", " Chapter 5 Re-analysis 3 This re-analysis is from the study Marton, Giulia, et al. “Patients’ health locus of control and preferences about the role that they want to play in the medical decision-making process.” Psychology, Health &amp; Medicine (2020): 1-7 library(bpcs) library(tidyverse) library(knitr) library(loo) set.seed(99) 5.1 Importing the data The data from this paper was made available upon request and below we exemplify a few rows of how the original dataset looks like. Let’s starting importing the data. d&lt;-read.table(&quot;data/MHLC.txt&quot;, sep=&quot;\\t&quot;, header=T) d&lt;-as.data.frame(d) sample_n(d, size=5) %&gt;% kable(caption = &#39;Sample of rows from the original data&#39;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 5.1: Sample of rows from the original data AB AC BC AD BD CD AE BE CE DE GENDER MHLC_INTERNAL MHLC_CHANCE MHLC_DOCTORS MHLC_OTHER_PEOPLE Age 1 1 1 1 1 1 1 1 1 1 2 16 14 12 10 56 1 1 1 1 1 0 0 0 0 0 2 23 13 14 7 53 1 1 1 0 0 0 0 0 0 0 1 19 6 9 5 58 1 1 1 1 1 0 1 0 0 0 2 19 7 7 8 27 1 1 1 1 0 0 0 0 0 0 1 14 12 11 5 28 As we can see, the data is in a wide format. Before we pivot it to longer let’s add a column that indicates the subject ID. d &lt;- d %&gt;% mutate(SubjectID = row_number()) Now let’s pivot it to the longer format cols_to_pivot&lt;-colnames(d)[1:10] d_longer&lt;-tidyr::pivot_longer(d, cols=all_of(cols_to_pivot), names_to=&#39;comparison&#39;, values_to=&#39;y&#39;) Now let’s divide the comparison into two vectors (choice0 and choice1). So it fits the bpcs format comp_cols &lt;- str_split_fixed(d_longer$comparison, &quot;&quot;, 2) d_longer$choice0 &lt;- comp_cols[,1] d_longer$choice1 &lt;- comp_cols[,2] The data frame now looks like this: dplyr::sample_n(d_longer, size=10) %&gt;% kable() %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) GENDER MHLC_INTERNAL MHLC_CHANCE MHLC_DOCTORS MHLC_OTHER_PEOPLE Age SubjectID comparison y choice0 choice1 1 22 6 15 14 31 96 BE 1 B E 2 20 17 8 9 26 125 DE 0 D E 2 30 30 15 15 27 116 AC 1 A C 2 23 8 14 9 55 36 BE 1 B E 2 23 18 13 11 53 42 CD 0 C D 1 20 17 14 9 37 79 AB 1 A B 2 21 23 13 11 78 2 DE 0 D E 2 20 23 8 13 30 100 CD 1 C D 2 19 19 17 9 34 93 AE 1 A E 2 33 31 8 13 45 58 DE 1 D E Now that we have setup the data frame correctly for the bpcs package, we can use it to model the problem. Let’s just rename a few values so it is easier to understand. #choice0 d_longer$choice0 &lt;- recode(d_longer$choice0, &#39;A&#39;=&#39;Active&#39;) d_longer$choice0 &lt;- recode(d_longer$choice0, &#39;B&#39;=&#39;Active-Collaborative&#39;) d_longer$choice0 &lt;- recode(d_longer$choice0, &#39;C&#39;=&#39;Collaborative&#39;) d_longer$choice0 &lt;- recode(d_longer$choice0, &#39;D&#39;=&#39;Passive-Collaborative&#39;) d_longer$choice0 &lt;- recode(d_longer$choice0, &#39;E&#39;=&#39;Passive&#39;) #choice1 d_longer$choice1 &lt;- recode(d_longer$choice1, &#39;A&#39;=&#39;Active&#39;) d_longer$choice1 &lt;- recode(d_longer$choice1, &#39;B&#39;=&#39;Active-Collaborative&#39;) d_longer$choice1 &lt;- recode(d_longer$choice1, &#39;C&#39;=&#39;Collaborative&#39;) d_longer$choice1 &lt;- recode(d_longer$choice1, &#39;D&#39;=&#39;Passive-Collaborative&#39;) d_longer$choice1 &lt;- recode(d_longer$choice1, &#39;E&#39;=&#39;Passive&#39;) Our final step in the preparation of the dataset is to standardize the values of the MHLOC scales. The values provided in the dataset correspond to the sum of the values of the scale and ranges from 3-18 or from 6-36. Therefore we will scale them accordingly for more stable inference d_longer &lt;- d_longer %&gt;% mutate(Internal = scale(MHLC_INTERNAL), Chance = scale(MHLC_CHANCE), Doctors = scale(MHLC_DOCTORS), OtherPeople = scale(MHLC_OTHER_PEOPLE)) The final modified dataset looks like: sample_n(d_longer, size=10) %&gt;% kable() %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) GENDER MHLC_INTERNAL MHLC_CHANCE MHLC_DOCTORS MHLC_OTHER_PEOPLE Age SubjectID comparison y choice0 choice1 Internal Chance Doctors OtherPeople 2 18 11 9 7 57 27 BD 1 Active-Collaborative Passive-Collaborative -0.4347094 -0.61218263 -0.76052743 -0.4611581 1 18 31 15 8 44 60 DE 0 Passive-Collaborative Passive -0.4347094 2.96961250 1.23422737 -0.1135857 2 15 17 15 9 27 119 AC 1 Active Collaborative -1.0563033 0.46235591 1.23422737 0.2339866 1 18 31 15 8 44 60 BE 0 Active-Collaborative Passive -0.4347094 2.96961250 1.23422737 -0.1135857 2 23 18 13 11 53 42 CD 0 Collaborative Passive-Collaborative 0.6012804 0.64144566 0.56930911 0.9291313 1 21 13 11 7 53 40 BE 0 Active-Collaborative Passive 0.1868844 -0.25400312 -0.09560916 -0.4611581 2 25 14 14 10 61 14 AC 1 Active Collaborative 1.0156763 -0.07491336 0.90176824 0.5815589 2 30 14 10 9 63 8 BE 0 Active-Collaborative Passive 2.0516661 -0.07491336 -0.42806830 0.2339866 2 15 15 12 8 36 87 BD 1 Active-Collaborative Passive-Collaborative -1.0563033 0.10417639 0.23684997 -0.1135857 1 15 14 9 6 NA 1 AE 0 Active Passive -1.0563033 -0.07491336 -0.76052743 -0.8087304 Now we can proceed with the analysis. 5.2 Simple Bradley-Terry models Let’s start with a simple BT without considering any subject predictors. Just to evaluate the average probability of people being Active, Active-Collaborative, Collaborative, Passive-Collaborative or Passive. m1 &lt;- bpc( d_longer, player0 = &#39;choice0&#39;, player1 = &#39;choice1&#39;, result_column = &#39;y&#39;, model_type = &#39;bt&#39;, priors = list(prior_lambda_std = 3.0), iter = 2000, show_chain_messages = T ) save_bpc_model(m1, &#39;m_hloc&#39;, &#39;./fittedmodels&#39;) Let’s investigate model convergence with shinystan and with check convergence function. Everything seems fine. launch_shinystan(m1) Now let’s get the waic m1_waic &lt;- get_waic(m1) m1_waic ## ## Computed from 8000 by 1530 log-likelihood matrix ## ## Estimate SE ## elpd_waic -711.3 22.2 ## p_waic 4.1 0.2 ## waic 1422.6 44.4 5.3 Subject predictors model We have different HLC that can be used as predictors for the response. Let’s create a single model with all the predictors. m2 &lt;- bpc( d_longer, player0 = &#39;choice0&#39;, player1 = &#39;choice1&#39;, result_column = &#39;y&#39;, subject_predictors = c(&#39;Internal&#39;, &#39;Chance&#39;, &#39;Doctors&#39;, &#39;OtherPeople&#39;), model_type = &#39;bt-subjectpredictors&#39;, priors = list(prior_lambda_std = 3.0, prior_S_std = 3.0), iter = 2000, show_chain_messages = T ) save_bpc_model(m2, &#39;m_mhloc_subjectpred&#39;, &#39;./fittedmodels&#39;) Diagnostics: launch_shinystan(m2) m2_waic &lt;- get_waic(m2) m2_waic ## ## Computed from 8000 by 1530 log-likelihood matrix ## ## Estimate SE ## elpd_waic -689.4 23.2 ## p_waic 20.8 1.1 ## waic 1378.7 46.3 5.4 Subject predictors and random effects Now let’s create a third model that also compensate for multiple judgment with a random effects variable. This model adds 153*4 variables, so sampling will take longer and be a bit more complex. m3 &lt;- bpc( d_longer, player0 = &#39;choice0&#39;, player1 = &#39;choice1&#39;, result_column = &#39;y&#39;, subject_predictors = c(&#39;Internal&#39;, &#39;Chance&#39;, &#39;Doctors&#39;, &#39;OtherPeople&#39;), cluster = c(&#39;SubjectID&#39;), model_type = &#39;bt-subjectpredictors-U&#39;, priors = list(prior_lambda_std = 3.0, prior_S_std = 3.0, prior_U1_std = 3.0), iter = 2000, show_chain_messages = T ) save_bpc_model(m3, &#39;m_mhloc_subjectpred_U&#39;, &#39;./fittedmodels&#39;) Diagnostics For a quick check check_convergence_diagnostics(m3) ## Processing csv files: /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106031852-1-1111fb.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106031852-2-1111fb.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106031852-3-1111fb.csv, /Users/davidis/Google Drive/Studies/PhD/papers/bpcs-appendix/.bpcs/bt-202106031852-4-1111fb.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory for all transitions. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. launch_shinystan(m3) m3_waic &lt;- get_waic(m3) m3_waic ## ## Computed from 8000 by 1530 log-likelihood matrix ## ## Estimate SE ## elpd_waic -400.9 17.7 ## p_waic 224.1 10.8 ## waic 801.8 35.4 ## ## 165 (10.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. 5.5 Comparing the WAIC Let’s compare the WAIC of the three models loo::loo_compare(m1_waic,m2_waic, m3_waic) ## elpd_diff se_diff ## model3 0.0 0.0 ## model2 -288.4 17.6 ## model1 -310.4 18.5 5.6 Plots and tables Now that we see that all models have proper convergence and that the model m3 has a better fit we will generate some plots and tables to help understand the problem lambda &lt;- get_parameters(m3, params = &#39;lambda&#39;, n_eff = T,keep_par_name = F) Spar &lt;- get_parameters(m3, params = &#39;S&#39;, n_eff = T) U_std &lt;- get_parameters(m3, params = &#39;U1_std&#39;, n_eff = T) Let’s create a custom table based on these parameters. But first we will rename the parameters a bit so the table reads a bit better. We are here removing the S[*] from the parameter Spar$Parameter &lt;- stringr::str_sub(Spar$Parameter,start = 3, end=-2) 5.6.1 Table lambda and U Now we can create the table #merge the datasets df_table &lt;- rbind(lambda, U_std) rownames(df_table)&lt;- NULL #creating the table df_table %&gt;% kable(caption = &#39;Lambda parameters of the model and the random effects standard deviation&#39;, digits = 2, col.names = c(&#39;Parameter&#39;, &#39;Mean&#39;,&#39;Median&#39;,&#39;HPD lower&#39;, &#39;HPD lower&#39;,&#39;N. Eff. Samples&#39;)) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 5.2: Lambda parameters of the model and the random effects standard deviation Parameter Mean Median HPD lower HPD lower N. Eff. Samples Active -3.16 -3.14 -5.98 -0.52 2793 Active-Collaborative 2.10 2.08 -0.60 4.82 2764 Collaborative 4.88 4.89 2.01 7.71 2741 Passive-Collaborative 1.23 1.24 -1.38 4.00 2724 Passive -5.11 -5.09 -7.99 -2.13 2718 U1_std 3.60 3.57 2.66 4.56 1899 5.6.2 Subject predictors table S &lt;- Spar %&gt;% tidyr::separate(Parameter,c(&#39;Role&#39;,&#39;MHLOC&#39;), sep=&quot;,&quot;) S$MHLOC &lt;- recode(S$MHLOC, &#39;OtherPeople&#39;= &#39;Other people&#39;) rownames(S) &lt;- NULL S %&gt;% dplyr::arrange(Role) %&gt;% select(-Role) %&gt;% kable(format=&#39;html&#39;, caption = &#39;Subject predictors parameters by role&#39;, digits = 2, col.names = c(&#39;Parameter&#39;, &#39;Mean&#39;, &#39;Median&#39;,&#39;HPD lower&#39;, &#39;HPD lower&#39;, &#39;N. Eff. Samples&#39;), booktabs=T) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::pack_rows(&quot;Active&quot;, 1, 4) %&gt;% kableExtra::pack_rows(&quot;Active-Collaborative&quot;, 5, 8) %&gt;% kableExtra::pack_rows(&quot;Collaborative&quot;, 9, 12) %&gt;% kableExtra::pack_rows(&quot;Passive-Collaborative&quot;, 13, 16) %&gt;% kableExtra::pack_rows(&quot;Passive&quot;, 17, 20) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 5.3: Subject predictors parameters by role Parameter Mean Median HPD lower HPD lower N. Eff. Samples Active Internal -0.16 -0.16 -2.72 2.61 2563 Chance -0.15 -0.15 -2.93 2.56 2494 Doctors -0.80 -0.80 -3.53 1.97 2211 Other people -0.29 -0.28 -3.16 2.33 2576 Active-Collaborative Internal -0.01 -0.01 -2.64 2.56 2534 Chance -0.24 -0.25 -2.94 2.55 2374 Doctors -0.74 -0.73 -3.50 1.92 2204 Other people -0.50 -0.50 -3.39 2.14 2594 Collaborative Internal -0.09 -0.08 -2.72 2.61 2571 Chance 0.09 0.09 -2.56 2.93 2362 Doctors 0.28 0.29 -2.48 3.02 2190 Other people -1.22 -1.23 -3.98 1.51 2703 Passive-Collaborative Internal 0.12 0.12 -2.46 2.90 2516 Chance 0.13 0.14 -2.60 2.90 2355 Doctors 0.75 0.76 -1.97 3.54 2174 Other people 1.04 1.04 -1.69 3.81 2708 Passive Internal 0.00 0.01 -2.71 2.59 2509 Chance -0.20 -0.20 -3.08 2.45 2375 Doctors 0.41 0.40 -2.38 3.12 2205 Other people 0.81 0.81 -1.90 3.59 2656 5.6.3 Plot S$Role &lt;- factor(S$Role, levels = c(&#39;Active&#39;, &#39;Active-Collaborative&#39;, &#39;Collaborative&#39;, &#39;Passive-Collaborative&#39;, &#39;Passive&#39;)) ggplot(S, aes(x=MHLOC))+ geom_pointrange(aes( ymin = HPD_lower, ymax = HPD_higher, y = Mean, group=MHLOC))+ facet_wrap(~Role, nrow = 3) + #Dividing the plot into three by species labs(title = &#39;Subject predictors estimates with the HPD interval&#39;, y = &#39;Estimate&#39;, x = &#39;HLOC dimension&#39;)+ theme_bw()+ # A black and white theme # scale_x_discrete(guide = guide_axis(n.dodge = 2)) + theme(legend.position=&quot;bottom&quot;) #small adjustments to the theme 5.6.4 Probability tables First let’s create a data frame with the cases we want to investigate. We will utilize the model_type option in the probabilities table so we can average out the values of the random effects in the subjects. In this table, we will only investigate a few of the conditions, but of course it is possible to do a much more expansive analysis Let’s create a new data frame with the conditions we want to investigate. Mainly we are just investigating: * Between the choice of Active and Passive how does going from -2 to 2 standard deviations over the mean influences the probability in the Internal and the Doctors * Between the choice of Active-Collaborative and Collaborative how does going from -2 to 2 standard deviations over the mean influences the probability in the Internal and the Doctors * Between the choice of Collaborative and Passive-Collaborative how does going from -2 to 2 standard deviations over the mean influences the probability in the Internal and the Doctors newdata &lt;- tibble::tribble(~choice1, ~choice0, ~Internal, ~Chance, ~Doctors, ~OtherPeople, &quot;Active&quot;, &quot;Passive&quot;, 0, 0, 0, 0, &quot;Active&quot;, &quot;Passive&quot;, -2, 0, 0, 0, &quot;Active&quot;, &quot;Passive&quot;, 2, 0, 0, 0, &quot;Active&quot;, &quot;Passive&quot;, 0, 0, -2, 0, &quot;Active&quot;, &quot;Passive&quot;, 0, 0, 2, 0, &quot;Active-Collaborative&quot;, &quot;Collaborative&quot;, 0, 0, 0, 0, &quot;Active-Collaborative&quot;, &quot;Collaborative&quot;, -2, 0, 0, 0, &quot;Active-Collaborative&quot;, &quot;Collaborative&quot;, 2, 0, 0, 0, &quot;Active-Collaborative&quot;, &quot;Collaborative&quot;, 0, 0, -2, 0, &quot;Active-Collaborative&quot;, &quot;Collaborative&quot;, 0, 0, 2, 0, &quot;Collaborative&quot;, &quot;Passive-Collaborative&quot;, 0, 0, 0, 0, &quot;Collaborative&quot;, &quot;Passive-Collaborative&quot;, 0, -2, 0, 0, &quot;Collaborative&quot;, &quot;Passive-Collaborative&quot;, 0, 2, 0, 0, &quot;Collaborative&quot;, &quot;Passive-Collaborative&quot;, 0, 0, 0, -2, &quot;Collaborative&quot;, &quot;Passive-Collaborative&quot;, 0, 0, 0, 2 ) #Now we can calculate the probabilities prob_hloc &lt;- get_probabilities_df( m3, newdata = newdata, model_type = &#39;bt-subjectpredictors&#39;) #here we are assuming zero for the random effects prob_hloc %&gt;% select(-j_beats_i) %&gt;% rename(Probability=i_beats_j) %&gt;% kable(caption = &quot;Probabilities of selecting role i instead of j based on changes of the values of the HLOC dimensions&quot;, digits = 2, booktabs = T) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;)) %&gt;% kableExtra::add_header_above(c(&quot;Roles&quot; = 2, &quot;HLOC dimensions&quot; = 4, &quot; &quot;=1)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Table 5.4: Probabilities of selecting role i instead of j based on changes of the values of the HLOC dimensions Roles HLOC dimensions i j Internal Chance Doctors OtherPeople Probability Active Passive 0 0 0 0 0.83 Active Passive -2 0 0 0 0.87 Active Passive 2 0 0 0 0.88 Active Passive 0 0 -2 0 0.83 Active Passive 0 0 2 0 0.86 Active-Collaborative Collaborative 0 0 0 0 0.08 Active-Collaborative Collaborative -2 0 0 0 0.07 Active-Collaborative Collaborative 2 0 0 0 0.05 Active-Collaborative Collaborative 0 0 -2 0 0.06 Active-Collaborative Collaborative 0 0 2 0 0.06 Collaborative Passive-Collaborative 0 0 0 0 0.99 Collaborative Passive-Collaborative 0 -2 0 0 0.96 Collaborative Passive-Collaborative 0 2 0 0 0.99 Collaborative Passive-Collaborative 0 0 0 -2 0.97 Collaborative Passive-Collaborative 0 0 0 2 0.93 "]]
